{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ObJAX_CIFAR10_example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxqOyLRaUkYW"
      },
      "source": [
        "# ObJAX CIFAR10 example\n",
        "\n",
        "This example is based on [cifar10_simple.py](https://github.com/google/objax/blob/master/examples/classify/img/cifar10_simple.py) with few minor changes:\n",
        "\n",
        "* it demonstrates how to do weight decay,\n",
        "* it uses Momentum optimizer with learning rate schedule,\n",
        "* it uses `tensorflow_datasets` instead of `Keras` dataset.\n",
        "\n",
        "It's recommended to run this notebook on GPU. In Google Colab this could be set through `Runtime -> Change runtime type` menu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYU-k0VsVRbL"
      },
      "source": [
        "# Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dg5WFQApM1_"
      },
      "source": [
        "%pip --quiet install objax"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oizwGGdIUMr0"
      },
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jn\n",
        "from jax.lax import lax\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import objax\n",
        "from objax.zoo.wide_resnet import WideResNet"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ65n3dQVvll"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnzqFRPpVwZ5"
      },
      "source": [
        "base_learning_rate = 0.1 # Learning rate\n",
        "lr_decay_epochs = 30     # How often to decay learning rate\n",
        "lr_decay_factor = 0.2    # By how much to decay learning rate\n",
        "weight_decay =  0.0005   # Weight decay\n",
        "batch_size = 128         # Batch size\n",
        "num_train_epochs = 100   # Number of training epochs\n",
        "wrn_width = 2            # Width of WideResNet\n",
        "wrn_depth = 28           # Depth of WideResNet"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu5ildnhWF3M"
      },
      "source": [
        "# Setup dataset and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBQ-xNSDWIsj"
      },
      "source": [
        "# Augmentation function for input data\n",
        "def augment(x):  # x is NCHW\n",
        "  \"\"\"Random flip and random shift augmentation of image batch.\"\"\"\n",
        "  if random.random() < .5:\n",
        "    x = x[:, :, :, ::-1]  # Flip the batch images about the horizontal axis\n",
        "  # Pixel-shift all images in the batch by up to 4 pixels in any direction.\n",
        "  x_pad = np.pad(x, [[0, 0], [0, 0], [4, 4], [4, 4]], 'reflect')\n",
        "  rx, ry = np.random.randint(0, 4), np.random.randint(0, 4)\n",
        "  x = x_pad[:, :, rx:rx + 32, ry:ry + 32]\n",
        "  return x\n",
        "\n",
        "# Data\n",
        "data = tfds.as_numpy(tfds.load(name='cifar10', batch_size=-1))\n",
        "x_train = data['train']['image'].transpose(0, 3, 1, 2) / 255.0\n",
        "y_train = data['train']['label']\n",
        "x_test = data['test']['image'].transpose(0, 3, 1, 2) / 255.0\n",
        "y_test = data['test']['label']\n",
        "del data\n",
        "\n",
        "# Model\n",
        "model = WideResNet(nin=3, nclass=10, depth=wrn_depth, width=wrn_width)\n",
        "model_vars = model.vars()\n",
        "weight_decay_vars = [v for k, v in model_vars.items() if k.endswith('.w')]\n",
        "\n",
        "# Optimizer\n",
        "opt = objax.optimizer.Momentum(model_vars, nesterov=True)\n",
        "\n",
        "# Prediction operation\n",
        "predict_op = lambda x: objax.functional.softmax(model(x, training=False))\n",
        "predict_op = objax.Jit(predict_op, model_vars)\n",
        "\n",
        "# Loss and training op\n",
        "def loss_fn(x, label):\n",
        "  logit = model(x, training=True)\n",
        "  xe_loss = objax.functional.loss.cross_entropy_logits_sparse(logit, label).mean()\n",
        "  wd_loss = sum((v.value ** 2).sum() for v in weight_decay_vars)\n",
        "  return xe_loss + weight_decay * wd_loss\n",
        "\n",
        "loss_gv = objax.GradValues(loss_fn, model.vars())\n",
        "\n",
        "def train_op(x, y, learning_rate):\n",
        "    grads, loss = loss_gv(x, y)\n",
        "    opt(learning_rate, grads)\n",
        "    return loss\n",
        "\n",
        "all_vars = model_vars + opt.vars()\n",
        "train_op = objax.Jit(train_op, all_vars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6KUOcDxC7gq"
      },
      "source": [
        "**Model parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMzCIgeJC-Wu",
        "outputId": "346b5364-0a56-4099-bbb9-78fcba908311",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(model_vars)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(WideResNet)[0](Conv2D).w                                        432 (3, 3, 3, 16)\n",
            "(WideResNet)[1](WRNBlock).proj_conv(Conv2D).w                    512 (1, 1, 16, 32)\n",
            "(WideResNet)[1](WRNBlock).norm_1(BatchNorm2D).running_mean        16 (1, 16, 1, 1)\n",
            "(WideResNet)[1](WRNBlock).norm_1(BatchNorm2D).running_var         16 (1, 16, 1, 1)\n",
            "(WideResNet)[1](WRNBlock).norm_1(BatchNorm2D).beta                16 (1, 16, 1, 1)\n",
            "(WideResNet)[1](WRNBlock).norm_1(BatchNorm2D).gamma               16 (1, 16, 1, 1)\n",
            "(WideResNet)[1](WRNBlock).conv_1(Conv2D).w                      4608 (3, 3, 16, 32)\n",
            "(WideResNet)[1](WRNBlock).norm_2(BatchNorm2D).running_mean        32 (1, 32, 1, 1)\n",
            "(WideResNet)[1](WRNBlock).norm_2(BatchNorm2D).running_var         32 (1, 32, 1, 1)\n",
            "(WideResNet)[1](WRNBlock).norm_2(BatchNorm2D).beta                32 (1, 32, 1, 1)\n",
            "(WideResNet)[1](WRNBlock).norm_2(BatchNorm2D).gamma               32 (1, 32, 1, 1)\n",
            "(WideResNet)[1](WRNBlock).conv_2(Conv2D).w                      9216 (3, 3, 32, 32)\n",
            "(WideResNet)[2](WRNBlock).norm_1(BatchNorm2D).running_mean        32 (1, 32, 1, 1)\n",
            "(WideResNet)[2](WRNBlock).norm_1(BatchNorm2D).running_var         32 (1, 32, 1, 1)\n",
            "(WideResNet)[2](WRNBlock).norm_1(BatchNorm2D).beta                32 (1, 32, 1, 1)\n",
            "(WideResNet)[2](WRNBlock).norm_1(BatchNorm2D).gamma               32 (1, 32, 1, 1)\n",
            "(WideResNet)[2](WRNBlock).conv_1(Conv2D).w                      9216 (3, 3, 32, 32)\n",
            "(WideResNet)[2](WRNBlock).norm_2(BatchNorm2D).running_mean        32 (1, 32, 1, 1)\n",
            "(WideResNet)[2](WRNBlock).norm_2(BatchNorm2D).running_var         32 (1, 32, 1, 1)\n",
            "(WideResNet)[2](WRNBlock).norm_2(BatchNorm2D).beta                32 (1, 32, 1, 1)\n",
            "(WideResNet)[2](WRNBlock).norm_2(BatchNorm2D).gamma               32 (1, 32, 1, 1)\n",
            "(WideResNet)[2](WRNBlock).conv_2(Conv2D).w                      9216 (3, 3, 32, 32)\n",
            "(WideResNet)[3](WRNBlock).norm_1(BatchNorm2D).running_mean        32 (1, 32, 1, 1)\n",
            "(WideResNet)[3](WRNBlock).norm_1(BatchNorm2D).running_var         32 (1, 32, 1, 1)\n",
            "(WideResNet)[3](WRNBlock).norm_1(BatchNorm2D).beta                32 (1, 32, 1, 1)\n",
            "(WideResNet)[3](WRNBlock).norm_1(BatchNorm2D).gamma               32 (1, 32, 1, 1)\n",
            "(WideResNet)[3](WRNBlock).conv_1(Conv2D).w                      9216 (3, 3, 32, 32)\n",
            "(WideResNet)[3](WRNBlock).norm_2(BatchNorm2D).running_mean        32 (1, 32, 1, 1)\n",
            "(WideResNet)[3](WRNBlock).norm_2(BatchNorm2D).running_var         32 (1, 32, 1, 1)\n",
            "(WideResNet)[3](WRNBlock).norm_2(BatchNorm2D).beta                32 (1, 32, 1, 1)\n",
            "(WideResNet)[3](WRNBlock).norm_2(BatchNorm2D).gamma               32 (1, 32, 1, 1)\n",
            "(WideResNet)[3](WRNBlock).conv_2(Conv2D).w                      9216 (3, 3, 32, 32)\n",
            "(WideResNet)[4](WRNBlock).norm_1(BatchNorm2D).running_mean        32 (1, 32, 1, 1)\n",
            "(WideResNet)[4](WRNBlock).norm_1(BatchNorm2D).running_var         32 (1, 32, 1, 1)\n",
            "(WideResNet)[4](WRNBlock).norm_1(BatchNorm2D).beta                32 (1, 32, 1, 1)\n",
            "(WideResNet)[4](WRNBlock).norm_1(BatchNorm2D).gamma               32 (1, 32, 1, 1)\n",
            "(WideResNet)[4](WRNBlock).conv_1(Conv2D).w                      9216 (3, 3, 32, 32)\n",
            "(WideResNet)[4](WRNBlock).norm_2(BatchNorm2D).running_mean        32 (1, 32, 1, 1)\n",
            "(WideResNet)[4](WRNBlock).norm_2(BatchNorm2D).running_var         32 (1, 32, 1, 1)\n",
            "(WideResNet)[4](WRNBlock).norm_2(BatchNorm2D).beta                32 (1, 32, 1, 1)\n",
            "(WideResNet)[4](WRNBlock).norm_2(BatchNorm2D).gamma               32 (1, 32, 1, 1)\n",
            "(WideResNet)[4](WRNBlock).conv_2(Conv2D).w                      9216 (3, 3, 32, 32)\n",
            "(WideResNet)[5](WRNBlock).proj_conv(Conv2D).w                   2048 (1, 1, 32, 64)\n",
            "(WideResNet)[5](WRNBlock).norm_1(BatchNorm2D).running_mean        32 (1, 32, 1, 1)\n",
            "(WideResNet)[5](WRNBlock).norm_1(BatchNorm2D).running_var         32 (1, 32, 1, 1)\n",
            "(WideResNet)[5](WRNBlock).norm_1(BatchNorm2D).beta                32 (1, 32, 1, 1)\n",
            "(WideResNet)[5](WRNBlock).norm_1(BatchNorm2D).gamma               32 (1, 32, 1, 1)\n",
            "(WideResNet)[5](WRNBlock).conv_1(Conv2D).w                     18432 (3, 3, 32, 64)\n",
            "(WideResNet)[5](WRNBlock).norm_2(BatchNorm2D).running_mean        64 (1, 64, 1, 1)\n",
            "(WideResNet)[5](WRNBlock).norm_2(BatchNorm2D).running_var         64 (1, 64, 1, 1)\n",
            "(WideResNet)[5](WRNBlock).norm_2(BatchNorm2D).beta                64 (1, 64, 1, 1)\n",
            "(WideResNet)[5](WRNBlock).norm_2(BatchNorm2D).gamma               64 (1, 64, 1, 1)\n",
            "(WideResNet)[5](WRNBlock).conv_2(Conv2D).w                     36864 (3, 3, 64, 64)\n",
            "(WideResNet)[6](WRNBlock).norm_1(BatchNorm2D).running_mean        64 (1, 64, 1, 1)\n",
            "(WideResNet)[6](WRNBlock).norm_1(BatchNorm2D).running_var         64 (1, 64, 1, 1)\n",
            "(WideResNet)[6](WRNBlock).norm_1(BatchNorm2D).beta                64 (1, 64, 1, 1)\n",
            "(WideResNet)[6](WRNBlock).norm_1(BatchNorm2D).gamma               64 (1, 64, 1, 1)\n",
            "(WideResNet)[6](WRNBlock).conv_1(Conv2D).w                     36864 (3, 3, 64, 64)\n",
            "(WideResNet)[6](WRNBlock).norm_2(BatchNorm2D).running_mean        64 (1, 64, 1, 1)\n",
            "(WideResNet)[6](WRNBlock).norm_2(BatchNorm2D).running_var         64 (1, 64, 1, 1)\n",
            "(WideResNet)[6](WRNBlock).norm_2(BatchNorm2D).beta                64 (1, 64, 1, 1)\n",
            "(WideResNet)[6](WRNBlock).norm_2(BatchNorm2D).gamma               64 (1, 64, 1, 1)\n",
            "(WideResNet)[6](WRNBlock).conv_2(Conv2D).w                     36864 (3, 3, 64, 64)\n",
            "(WideResNet)[7](WRNBlock).norm_1(BatchNorm2D).running_mean        64 (1, 64, 1, 1)\n",
            "(WideResNet)[7](WRNBlock).norm_1(BatchNorm2D).running_var         64 (1, 64, 1, 1)\n",
            "(WideResNet)[7](WRNBlock).norm_1(BatchNorm2D).beta                64 (1, 64, 1, 1)\n",
            "(WideResNet)[7](WRNBlock).norm_1(BatchNorm2D).gamma               64 (1, 64, 1, 1)\n",
            "(WideResNet)[7](WRNBlock).conv_1(Conv2D).w                     36864 (3, 3, 64, 64)\n",
            "(WideResNet)[7](WRNBlock).norm_2(BatchNorm2D).running_mean        64 (1, 64, 1, 1)\n",
            "(WideResNet)[7](WRNBlock).norm_2(BatchNorm2D).running_var         64 (1, 64, 1, 1)\n",
            "(WideResNet)[7](WRNBlock).norm_2(BatchNorm2D).beta                64 (1, 64, 1, 1)\n",
            "(WideResNet)[7](WRNBlock).norm_2(BatchNorm2D).gamma               64 (1, 64, 1, 1)\n",
            "(WideResNet)[7](WRNBlock).conv_2(Conv2D).w                     36864 (3, 3, 64, 64)\n",
            "(WideResNet)[8](WRNBlock).norm_1(BatchNorm2D).running_mean        64 (1, 64, 1, 1)\n",
            "(WideResNet)[8](WRNBlock).norm_1(BatchNorm2D).running_var         64 (1, 64, 1, 1)\n",
            "(WideResNet)[8](WRNBlock).norm_1(BatchNorm2D).beta                64 (1, 64, 1, 1)\n",
            "(WideResNet)[8](WRNBlock).norm_1(BatchNorm2D).gamma               64 (1, 64, 1, 1)\n",
            "(WideResNet)[8](WRNBlock).conv_1(Conv2D).w                     36864 (3, 3, 64, 64)\n",
            "(WideResNet)[8](WRNBlock).norm_2(BatchNorm2D).running_mean        64 (1, 64, 1, 1)\n",
            "(WideResNet)[8](WRNBlock).norm_2(BatchNorm2D).running_var         64 (1, 64, 1, 1)\n",
            "(WideResNet)[8](WRNBlock).norm_2(BatchNorm2D).beta                64 (1, 64, 1, 1)\n",
            "(WideResNet)[8](WRNBlock).norm_2(BatchNorm2D).gamma               64 (1, 64, 1, 1)\n",
            "(WideResNet)[8](WRNBlock).conv_2(Conv2D).w                     36864 (3, 3, 64, 64)\n",
            "(WideResNet)[9](WRNBlock).proj_conv(Conv2D).w                   8192 (1, 1, 64, 128)\n",
            "(WideResNet)[9](WRNBlock).norm_1(BatchNorm2D).running_mean        64 (1, 64, 1, 1)\n",
            "(WideResNet)[9](WRNBlock).norm_1(BatchNorm2D).running_var         64 (1, 64, 1, 1)\n",
            "(WideResNet)[9](WRNBlock).norm_1(BatchNorm2D).beta                64 (1, 64, 1, 1)\n",
            "(WideResNet)[9](WRNBlock).norm_1(BatchNorm2D).gamma               64 (1, 64, 1, 1)\n",
            "(WideResNet)[9](WRNBlock).conv_1(Conv2D).w                     73728 (3, 3, 64, 128)\n",
            "(WideResNet)[9](WRNBlock).norm_2(BatchNorm2D).running_mean       128 (1, 128, 1, 1)\n",
            "(WideResNet)[9](WRNBlock).norm_2(BatchNorm2D).running_var        128 (1, 128, 1, 1)\n",
            "(WideResNet)[9](WRNBlock).norm_2(BatchNorm2D).beta               128 (1, 128, 1, 1)\n",
            "(WideResNet)[9](WRNBlock).norm_2(BatchNorm2D).gamma              128 (1, 128, 1, 1)\n",
            "(WideResNet)[9](WRNBlock).conv_2(Conv2D).w                    147456 (3, 3, 128, 128)\n",
            "(WideResNet)[10](WRNBlock).norm_1(BatchNorm2D).running_mean      128 (1, 128, 1, 1)\n",
            "(WideResNet)[10](WRNBlock).norm_1(BatchNorm2D).running_var       128 (1, 128, 1, 1)\n",
            "(WideResNet)[10](WRNBlock).norm_1(BatchNorm2D).beta              128 (1, 128, 1, 1)\n",
            "(WideResNet)[10](WRNBlock).norm_1(BatchNorm2D).gamma             128 (1, 128, 1, 1)\n",
            "(WideResNet)[10](WRNBlock).conv_1(Conv2D).w                   147456 (3, 3, 128, 128)\n",
            "(WideResNet)[10](WRNBlock).norm_2(BatchNorm2D).running_mean      128 (1, 128, 1, 1)\n",
            "(WideResNet)[10](WRNBlock).norm_2(BatchNorm2D).running_var       128 (1, 128, 1, 1)\n",
            "(WideResNet)[10](WRNBlock).norm_2(BatchNorm2D).beta              128 (1, 128, 1, 1)\n",
            "(WideResNet)[10](WRNBlock).norm_2(BatchNorm2D).gamma             128 (1, 128, 1, 1)\n",
            "(WideResNet)[10](WRNBlock).conv_2(Conv2D).w                   147456 (3, 3, 128, 128)\n",
            "(WideResNet)[11](WRNBlock).norm_1(BatchNorm2D).running_mean      128 (1, 128, 1, 1)\n",
            "(WideResNet)[11](WRNBlock).norm_1(BatchNorm2D).running_var       128 (1, 128, 1, 1)\n",
            "(WideResNet)[11](WRNBlock).norm_1(BatchNorm2D).beta              128 (1, 128, 1, 1)\n",
            "(WideResNet)[11](WRNBlock).norm_1(BatchNorm2D).gamma             128 (1, 128, 1, 1)\n",
            "(WideResNet)[11](WRNBlock).conv_1(Conv2D).w                   147456 (3, 3, 128, 128)\n",
            "(WideResNet)[11](WRNBlock).norm_2(BatchNorm2D).running_mean      128 (1, 128, 1, 1)\n",
            "(WideResNet)[11](WRNBlock).norm_2(BatchNorm2D).running_var       128 (1, 128, 1, 1)\n",
            "(WideResNet)[11](WRNBlock).norm_2(BatchNorm2D).beta              128 (1, 128, 1, 1)\n",
            "(WideResNet)[11](WRNBlock).norm_2(BatchNorm2D).gamma             128 (1, 128, 1, 1)\n",
            "(WideResNet)[11](WRNBlock).conv_2(Conv2D).w                   147456 (3, 3, 128, 128)\n",
            "(WideResNet)[12](WRNBlock).norm_1(BatchNorm2D).running_mean      128 (1, 128, 1, 1)\n",
            "(WideResNet)[12](WRNBlock).norm_1(BatchNorm2D).running_var       128 (1, 128, 1, 1)\n",
            "(WideResNet)[12](WRNBlock).norm_1(BatchNorm2D).beta              128 (1, 128, 1, 1)\n",
            "(WideResNet)[12](WRNBlock).norm_1(BatchNorm2D).gamma             128 (1, 128, 1, 1)\n",
            "(WideResNet)[12](WRNBlock).conv_1(Conv2D).w                   147456 (3, 3, 128, 128)\n",
            "(WideResNet)[12](WRNBlock).norm_2(BatchNorm2D).running_mean      128 (1, 128, 1, 1)\n",
            "(WideResNet)[12](WRNBlock).norm_2(BatchNorm2D).running_var       128 (1, 128, 1, 1)\n",
            "(WideResNet)[12](WRNBlock).norm_2(BatchNorm2D).beta              128 (1, 128, 1, 1)\n",
            "(WideResNet)[12](WRNBlock).norm_2(BatchNorm2D).gamma             128 (1, 128, 1, 1)\n",
            "(WideResNet)[12](WRNBlock).conv_2(Conv2D).w                   147456 (3, 3, 128, 128)\n",
            "(WideResNet)[13](BatchNorm2D).running_mean                       128 (1, 128, 1, 1)\n",
            "(WideResNet)[13](BatchNorm2D).running_var                        128 (1, 128, 1, 1)\n",
            "(WideResNet)[13](BatchNorm2D).beta                               128 (1, 128, 1, 1)\n",
            "(WideResNet)[13](BatchNorm2D).gamma                              128 (1, 128, 1, 1)\n",
            "(WideResNet)[16](Linear).b                                        10 (10,)\n",
            "(WideResNet)[16](Linear).w                                      1280 (128, 10)\n",
            "+Total(130)                                                  1471226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQH7UyxxWf9j"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgnrJyv0Whah",
        "outputId": "bcb5a5eb-8c0f-4b42-bf24-fe35a1611097",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "  return base_learning_rate * math.pow(lr_decay_factor, epoch // lr_decay_epochs)\n",
        "\n",
        "num_train_examples = x_train.shape[0]\n",
        "num_test_examples = x_test.shape[0]\n",
        "for epoch in range(num_train_epochs):\n",
        "  # Training\n",
        "  example_indices = np.arange(num_train_examples)\n",
        "  np.random.shuffle(example_indices)\n",
        "  for idx in range(0, num_train_examples, batch_size):\n",
        "    x = x_train[example_indices[idx:idx + batch_size]]\n",
        "    y = y_train[example_indices[idx:idx + batch_size]]\n",
        "    loss = train_op(augment(x), y, lr_schedule(epoch))[0]\n",
        "\n",
        "  # Eval\n",
        "  accuracy = 0\n",
        "  for idx in range(0, num_test_examples, batch_size):\n",
        "    x = x_test[idx:idx + batch_size]\n",
        "    y = y_test[idx:idx + batch_size]\n",
        "    p = predict_op(x)\n",
        "    accuracy += (np.argmax(p, axis=1) == y).sum()\n",
        "  accuracy /= num_test_examples\n",
        "  print(f'Epoch {epoch+1:3} -- train loss {loss:.3f}   test accuracy {accuracy*100:.1f}', flush=True)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   1 -- train loss 1.889   test accuracy 57.9\n",
            "Epoch   2 -- train loss 1.767   test accuracy 68.8\n",
            "Epoch   3 -- train loss 0.990   test accuracy 62.2\n",
            "Epoch   4 -- train loss 0.977   test accuracy 68.4\n",
            "Epoch   5 -- train loss 0.838   test accuracy 72.2\n",
            "Epoch   6 -- train loss 1.179   test accuracy 72.3\n",
            "Epoch   7 -- train loss 1.019   test accuracy 70.7\n",
            "Epoch   8 -- train loss 0.811   test accuracy 78.2\n",
            "Epoch   9 -- train loss 1.050   test accuracy 78.5\n",
            "Epoch  10 -- train loss 0.947   test accuracy 77.3\n",
            "Epoch  11 -- train loss 0.721   test accuracy 78.1\n",
            "Epoch  12 -- train loss 0.743   test accuracy 74.6\n",
            "Epoch  13 -- train loss 0.738   test accuracy 68.4\n",
            "Epoch  14 -- train loss 0.909   test accuracy 79.3\n",
            "Epoch  15 -- train loss 0.804   test accuracy 74.2\n",
            "Epoch  16 -- train loss 0.829   test accuracy 74.5\n",
            "Epoch  17 -- train loss 0.764   test accuracy 76.6\n",
            "Epoch  18 -- train loss 0.845   test accuracy 79.7\n",
            "Epoch  19 -- train loss 0.917   test accuracy 74.1\n",
            "Epoch  20 -- train loss 0.709   test accuracy 77.6\n",
            "Epoch  21 -- train loss 0.698   test accuracy 74.9\n",
            "Epoch  22 -- train loss 0.694   test accuracy 78.0\n",
            "Epoch  23 -- train loss 0.764   test accuracy 79.4\n",
            "Epoch  24 -- train loss 0.829   test accuracy 79.0\n",
            "Epoch  25 -- train loss 0.960   test accuracy 75.9\n",
            "Epoch  26 -- train loss 0.961   test accuracy 81.2\n",
            "Epoch  27 -- train loss 0.697   test accuracy 78.3\n",
            "Epoch  28 -- train loss 0.805   test accuracy 73.5\n",
            "Epoch  29 -- train loss 0.783   test accuracy 65.1\n",
            "Epoch  30 -- train loss 1.092   test accuracy 81.6\n",
            "Epoch  31 -- train loss 0.503   test accuracy 90.0\n",
            "Epoch  32 -- train loss 0.505   test accuracy 90.3\n",
            "Epoch  33 -- train loss 0.514   test accuracy 89.1\n",
            "Epoch  34 -- train loss 0.418   test accuracy 89.3\n",
            "Epoch  35 -- train loss 0.556   test accuracy 89.1\n",
            "Epoch  36 -- train loss 0.414   test accuracy 89.0\n",
            "Epoch  37 -- train loss 0.364   test accuracy 89.2\n",
            "Epoch  38 -- train loss 0.425   test accuracy 88.7\n",
            "Epoch  39 -- train loss 0.482   test accuracy 87.9\n",
            "Epoch  40 -- train loss 0.377   test accuracy 89.5\n",
            "Epoch  41 -- train loss 0.314   test accuracy 87.3\n",
            "Epoch  42 -- train loss 0.463   test accuracy 87.0\n",
            "Epoch  43 -- train loss 0.374   test accuracy 88.1\n",
            "Epoch  44 -- train loss 0.496   test accuracy 87.6\n",
            "Epoch  45 -- train loss 0.390   test accuracy 87.5\n",
            "Epoch  46 -- train loss 0.466   test accuracy 88.9\n",
            "Epoch  47 -- train loss 0.374   test accuracy 88.6\n",
            "Epoch  48 -- train loss 0.424   test accuracy 88.6\n",
            "Epoch  49 -- train loss 0.423   test accuracy 86.2\n",
            "Epoch  50 -- train loss 0.421   test accuracy 84.2\n",
            "Epoch  51 -- train loss 0.410   test accuracy 89.2\n",
            "Epoch  52 -- train loss 0.479   test accuracy 87.9\n",
            "Epoch  53 -- train loss 0.469   test accuracy 88.9\n",
            "Epoch  54 -- train loss 0.467   test accuracy 87.5\n",
            "Epoch  55 -- train loss 0.486   test accuracy 86.9\n",
            "Epoch  56 -- train loss 0.418   test accuracy 88.2\n",
            "Epoch  57 -- train loss 0.467   test accuracy 88.2\n",
            "Epoch  58 -- train loss 0.536   test accuracy 88.1\n",
            "Epoch  59 -- train loss 0.457   test accuracy 88.4\n",
            "Epoch  60 -- train loss 0.477   test accuracy 87.5\n",
            "Epoch  61 -- train loss 0.324   test accuracy 92.1\n",
            "Epoch  62 -- train loss 0.274   test accuracy 92.5\n",
            "Epoch  63 -- train loss 0.265   test accuracy 92.6\n",
            "Epoch  64 -- train loss 0.243   test accuracy 92.5\n",
            "Epoch  65 -- train loss 0.234   test accuracy 92.7\n",
            "Epoch  66 -- train loss 0.230   test accuracy 92.5\n",
            "Epoch  67 -- train loss 0.222   test accuracy 92.7\n",
            "Epoch  68 -- train loss 0.212   test accuracy 92.6\n",
            "Epoch  69 -- train loss 0.216   test accuracy 92.7\n",
            "Epoch  70 -- train loss 0.233   test accuracy 92.7\n",
            "Epoch  71 -- train loss 0.215   test accuracy 92.6\n",
            "Epoch  72 -- train loss 0.190   test accuracy 92.6\n",
            "Epoch  73 -- train loss 0.226   test accuracy 92.2\n",
            "Epoch  74 -- train loss 0.185   test accuracy 92.4\n",
            "Epoch  75 -- train loss 0.221   test accuracy 92.4\n",
            "Epoch  76 -- train loss 0.173   test accuracy 92.2\n",
            "Epoch  77 -- train loss 0.170   test accuracy 92.2\n",
            "Epoch  78 -- train loss 0.167   test accuracy 92.2\n",
            "Epoch  79 -- train loss 0.165   test accuracy 92.3\n",
            "Epoch  80 -- train loss 0.164   test accuracy 91.6\n",
            "Epoch  81 -- train loss 0.151   test accuracy 91.6\n",
            "Epoch  82 -- train loss 0.200   test accuracy 92.0\n",
            "Epoch  83 -- train loss 0.177   test accuracy 91.9\n",
            "Epoch  84 -- train loss 0.176   test accuracy 91.9\n",
            "Epoch  85 -- train loss 0.165   test accuracy 92.4\n",
            "Epoch  86 -- train loss 0.247   test accuracy 91.9\n",
            "Epoch  87 -- train loss 0.184   test accuracy 91.6\n",
            "Epoch  88 -- train loss 0.157   test accuracy 91.6\n",
            "Epoch  89 -- train loss 0.187   test accuracy 91.7\n",
            "Epoch  90 -- train loss 0.139   test accuracy 91.9\n",
            "Epoch  91 -- train loss 0.136   test accuracy 92.6\n",
            "Epoch  92 -- train loss 0.162   test accuracy 92.7\n",
            "Epoch  93 -- train loss 0.128   test accuracy 92.8\n",
            "Epoch  94 -- train loss 0.139   test accuracy 92.9\n",
            "Epoch  95 -- train loss 0.130   test accuracy 92.8\n",
            "Epoch  96 -- train loss 0.126   test accuracy 92.9\n",
            "Epoch  97 -- train loss 0.145   test accuracy 92.8\n",
            "Epoch  98 -- train loss 0.124   test accuracy 92.9\n",
            "Epoch  99 -- train loss 0.128   test accuracy 93.0\n",
            "Epoch 100 -- train loss 0.124   test accuracy 93.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
