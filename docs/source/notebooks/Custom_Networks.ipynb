{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom-Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jZgxBq4s3ndn",
        "0klE2QptdcaG",
        "an_z0dDmflp1",
        "lnrHPajlwHHM",
        "G31YQExqwNNc",
        "r9WgeoI3wo-o"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_76JLWRxku5p",
        "colab_type": "text"
      },
      "source": [
        "# Creating Custom Networks for Multi-Class Classification\n",
        "\n",
        "This tutorial demonstrates how to define, train, and use different models for multi-class classification. We will reuse most of the code from the [Logistic Regression](Logistic_Regression.html) tutorial so if you haven't gone through that, consider reviewing it first. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by3H5pItdUj8",
        "colab_type": "text"
      },
      "source": [
        "## Import Modules\n",
        "\n",
        "We start by importing the modules we will use in our code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4xxyuS0czl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pip --quiet install objax\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import objax\n",
        "from objax.util import EasyDict\n",
        "from objax.zoo.dnnet import DNNet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiMCbnmtdKAE",
        "colab_type": "text"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "Next, we will load the \"[MNIST](http://yann.lecun.com/exdb/mnist/)\" dataset from [TensorFlow DataSets](https://www.tensorflow.org/datasets/api_docs/python/tfds). This dataset contains handwritten digits (i.e., numbers between 0 and 9) and to correctly identify each handwritten digit. \n",
        "\n",
        "The ``prepare`` method pads 2 pixels to the left, right, top, and bottom of each image to resize into 32 x 32 pixes. While MNIST images are grayscale the 'prepare` method expands each image to three color channels to demonstrate the process of working with color images. The same method also rescales each pixel value to [-1, 1], and converts the image to (N, C, H, W) format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEBT7ZTGdJb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data: train has 60000 images - test has 10000 images\n",
        "# Each image is resized and converted to 32 x 32 x 3\n",
        "DATA_DIR = os.path.join(os.environ['HOME'], 'TFDS')\n",
        "data = tfds.as_numpy(tfds.load(name='mnist', batch_size=-1, data_dir=DATA_DIR))\n",
        "\n",
        "def prepare(x):\n",
        "  \"\"\"Pads 2 pixels to the left, right, top, and bottom of each image, scales pixel value to [-1, 1], and converts to NCHW format.\"\"\"\n",
        "  s = x.shape\n",
        "  x_pad = np.zeros((s[0], 32, 32, 1))\n",
        "  x_pad[:, 2:-2, 2:-2, :] = x\n",
        "  return objax.util.image.nchw(\n",
        "      np.concatenate([x_pad.astype('f') * (1 / 127.5) - 1] * 3, axis=-1))\n",
        "\n",
        "train = EasyDict(image=prepare(data['train']['image']), label=data['train']['label'])\n",
        "test = EasyDict(image=prepare(data['test']['image']), label=data['test']['label'])\n",
        "ndim = train.image.shape[-1]\n",
        "\n",
        "del data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZgxBq4s3ndn",
        "colab_type": "text"
      },
      "source": [
        "## Deep Neural Network Model\n",
        "\n",
        "Objax offers many predefined models that we can use for classification. One example is the ``objax.zoo.DNNet`` model comprising multiple fully connected layers with configurable size and activation functions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KeRqkHq3nFg",
        "colab_type": "code",
        "colab": {
          "height": 70
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1597866618900,
          "user_tz": 420,
          "elapsed": 1245,
          "user": {
            "displayName": "Alex Kurakin",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggxs3OlaNvrJx7N9pYPsTha3Rztq6Jr89dSgPKY=s64",
            "userId": "14505005774567669519"
          }
        },
        "outputId": "ce6436d7-9604-44d1-aa97-ec319e3aac5e"
      },
      "source": [
        "dnn_layer_sizes = 3072, 128, 10\n",
        "dnn_model = DNNet(dnn_layer_sizes, objax.functional.leaky_relu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0klE2QptdcaG",
        "colab_type": "text"
      },
      "source": [
        "## Custom Model Definition\n",
        "\n",
        "Alternatively, we can define a new model customized to our machine learning task. We demonstrate this process by defining a convolutional network (ConvNet) from scratch. \n",
        "\n",
        "We use ``objax.nn.Sequential`` to compose multiple layers of convolution (``objax.nn.Conv2D``), batch normalization (``objax.nn.BatchNorm2D``), ReLU (``objax.functional.relu``), Max Pooling (``objax.functional.max_pool_2d``), Average Pooling (``jax.mean``), and Linear (``objax.nn.Linear``) layers.\n",
        "\n",
        "Since [batch normalization layer](https://arxiv.org/abs/1502.03167) behaves differently at training and at prediction, we pass the ``training`` flag to a ``__call__`` function of ``ConvNet`` class. We also use the flag to output logits at training and probability at prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqvdMVI75fl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvNet(objax.Module):\n",
        "  \"\"\"ConvNet implementation.\"\"\"\n",
        "\n",
        "  def __init__(self, nin, nclass):\n",
        "    \"\"\"Define 3 blocks of conv-bn-relu-conv-bn-relu followed by linear layer.\"\"\"\n",
        "    self.conv_block1 = objax.nn.Sequential([objax.nn.Conv2D(nin, 16, 3, use_bias=False),\n",
        "                                            objax.nn.BatchNorm2D(16),\n",
        "                                            objax.functional.relu,\n",
        "                                            objax.nn.Conv2D(16, 16, 3, use_bias=False),\n",
        "                                            objax.nn.BatchNorm2D(16),\n",
        "                                            objax.functional.relu])\n",
        "    self.conv_block2 = objax.nn.Sequential([objax.nn.Conv2D(16, 32, 3, use_bias=False),\n",
        "                                            objax.nn.BatchNorm2D(32),\n",
        "                                            objax.functional.relu,\n",
        "                                            objax.nn.Conv2D(32, 32, 3, use_bias=False),\n",
        "                                            objax.nn.BatchNorm2D(32),\n",
        "                                            objax.functional.relu])\n",
        "    self.conv_block3 = objax.nn.Sequential([objax.nn.Conv2D(32, 64, 3, use_bias=False),\n",
        "                                            objax.nn.BatchNorm2D(64),\n",
        "                                            objax.functional.relu,\n",
        "                                            objax.nn.Conv2D(64, 64, 3, use_bias=False),\n",
        "                                            objax.nn.BatchNorm2D(64),\n",
        "                                            objax.functional.relu])\n",
        "    self.linear = objax.nn.Linear(64, nclass)\n",
        "\n",
        "  def __call__(self, x, training):\n",
        "    x = self.conv_block1(x, training=training)\n",
        "    x = objax.functional.max_pool_2d(x, size=2, strides=2)\n",
        "    x = self.conv_block2(x, training=training)\n",
        "    x = objax.functional.max_pool_2d(x, size=2, strides=2)\n",
        "    x = self.conv_block3(x, training=training)\n",
        "    x = x.mean((2, 3))\n",
        "    x = self.linear(x)\n",
        "    return x\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMrrT3SudevT",
        "colab_type": "code",
        "colab": {
          "height": 571
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1597866622579,
          "user_tz": 420,
          "elapsed": 3362,
          "user": {
            "displayName": "Alex Kurakin",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggxs3OlaNvrJx7N9pYPsTha3Rztq6Jr89dSgPKY=s64",
            "userId": "14505005774567669519"
          }
        },
        "outputId": "170ed2d8-7c35-450a-9c63-d6b7948890ac"
      },
      "source": [
        "cnn_model = ConvNet(nin=3, nclass=10)\n",
        "cnn_model.vars().print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(ConvNet).conv_block1(Sequential)[0](Conv2D).w                    432 (3, 3, 3, 16)\n",
            "(ConvNet).conv_block1(Sequential)[1](BatchNorm2D).running_mean       16 (1, 16, 1, 1)\n",
            "(ConvNet).conv_block1(Sequential)[1](BatchNorm2D).running_var       16 (1, 16, 1, 1)\n",
            "(ConvNet).conv_block1(Sequential)[1](BatchNorm2D).beta             16 (1, 16, 1, 1)\n",
            "(ConvNet).conv_block1(Sequential)[1](BatchNorm2D).gamma            16 (1, 16, 1, 1)\n",
            "(ConvNet).conv_block1(Sequential)[3](Conv2D).w                   2304 (3, 3, 16, 16)\n",
            "(ConvNet).conv_block1(Sequential)[4](BatchNorm2D).running_mean       16 (1, 16, 1, 1)\n",
            "(ConvNet).conv_block1(Sequential)[4](BatchNorm2D).running_var       16 (1, 16, 1, 1)\n",
            "(ConvNet).conv_block1(Sequential)[4](BatchNorm2D).beta             16 (1, 16, 1, 1)\n",
            "(ConvNet).conv_block1(Sequential)[4](BatchNorm2D).gamma            16 (1, 16, 1, 1)\n",
            "(ConvNet).conv_block2(Sequential)[0](Conv2D).w                   4608 (3, 3, 16, 32)\n",
            "(ConvNet).conv_block2(Sequential)[1](BatchNorm2D).running_mean       32 (1, 32, 1, 1)\n",
            "(ConvNet).conv_block2(Sequential)[1](BatchNorm2D).running_var       32 (1, 32, 1, 1)\n",
            "(ConvNet).conv_block2(Sequential)[1](BatchNorm2D).beta             32 (1, 32, 1, 1)\n",
            "(ConvNet).conv_block2(Sequential)[1](BatchNorm2D).gamma            32 (1, 32, 1, 1)\n",
            "(ConvNet).conv_block2(Sequential)[3](Conv2D).w                   9216 (3, 3, 32, 32)\n",
            "(ConvNet).conv_block2(Sequential)[4](BatchNorm2D).running_mean       32 (1, 32, 1, 1)\n",
            "(ConvNet).conv_block2(Sequential)[4](BatchNorm2D).running_var       32 (1, 32, 1, 1)\n",
            "(ConvNet).conv_block2(Sequential)[4](BatchNorm2D).beta             32 (1, 32, 1, 1)\n",
            "(ConvNet).conv_block2(Sequential)[4](BatchNorm2D).gamma            32 (1, 32, 1, 1)\n",
            "(ConvNet).conv_block3(Sequential)[0](Conv2D).w                  18432 (3, 3, 32, 64)\n",
            "(ConvNet).conv_block3(Sequential)[1](BatchNorm2D).running_mean       64 (1, 64, 1, 1)\n",
            "(ConvNet).conv_block3(Sequential)[1](BatchNorm2D).running_var       64 (1, 64, 1, 1)\n",
            "(ConvNet).conv_block3(Sequential)[1](BatchNorm2D).beta             64 (1, 64, 1, 1)\n",
            "(ConvNet).conv_block3(Sequential)[1](BatchNorm2D).gamma            64 (1, 64, 1, 1)\n",
            "(ConvNet).conv_block3(Sequential)[3](Conv2D).w                  36864 (3, 3, 64, 64)\n",
            "(ConvNet).conv_block3(Sequential)[4](BatchNorm2D).running_mean       64 (1, 64, 1, 1)\n",
            "(ConvNet).conv_block3(Sequential)[4](BatchNorm2D).running_var       64 (1, 64, 1, 1)\n",
            "(ConvNet).conv_block3(Sequential)[4](BatchNorm2D).beta             64 (1, 64, 1, 1)\n",
            "(ConvNet).conv_block3(Sequential)[4](BatchNorm2D).gamma            64 (1, 64, 1, 1)\n",
            "(ConvNet).linear(Linear).b                                         10 (10,)\n",
            "(ConvNet).linear(Linear).w                                        640 (64, 10)\n",
            "+Total(32)                                                      73402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an_z0dDmflp1",
        "colab_type": "text"
      },
      "source": [
        "## Model Training and Evaluation\n",
        "\n",
        "The ``train_model`` method combines all the parts of defining the loss function, gradient descent, training loop, and evaluation. It takes the ``model`` as a parameter so it can be reused with the two models we defined earlier.\n",
        "\n",
        "Unlike the Logistic Regression tutorial we use the ``objax.functional.loss.cross_entropy_logits_sparse`` because we perform multi-class classification. The optimizer, gradient descent operation, and training loop remain the same. \n",
        "\n",
        "The ``DNNet`` model expects flattened images whereas ``ConvNet`` images in (C, H, W) format. The ``flatten_image`` method prepares images before passing them to the model. \n",
        "\n",
        "When using the model for inference we apply the ``objax.functional.softmax`` method to compute the probability distribution from the model's logits. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN7aRrzw6OnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Settings\n",
        "lr = 0.03  # learning rate\n",
        "batch = 128\n",
        "epochs = 100\n",
        "\n",
        "# Train loop\n",
        "\n",
        "def train_model(model):\n",
        "\n",
        "  def predict(model, x):\n",
        "    \"\"\"\"\"\" \n",
        "    return objax.functional.softmax(model(x,  training=False))\n",
        "    \n",
        "  def flatten_image(x):\n",
        "    \"\"\"Flatten the image before passing it to the DNN.\"\"\"\n",
        "    if isinstance(model, DNNet):\n",
        "      return objax.functional.flatten(x)\n",
        "    else:\n",
        "      return x\n",
        "  \n",
        "  opt = objax.optimizer.Momentum(model.vars())\n",
        "\n",
        "  # Cross Entropy Loss\n",
        "  def loss(x, label):\n",
        "    return objax.functional.loss.cross_entropy_logits_sparse(model(x, training=True), label).mean()\n",
        "\n",
        "  gv = objax.GradValues(loss, model.vars())\n",
        "  def train_op(x, label):\n",
        "    g, v = gv(x, label)  # returns gradients, loss\n",
        "    opt(lr, g)\n",
        "    return v\n",
        "\n",
        "  train_op = objax.Jit(train_op, gv.vars() + opt.vars())  \n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    avg_loss = 0\n",
        "    # randomly shuffle training data\n",
        "    shuffle_idx = np.random.permutation(train.image.shape[0])\n",
        "    for it in range(0, train.image.shape[0], batch):\n",
        "      sel = shuffle_idx[it: it + batch]\n",
        "      avg_loss += float(train_op(flatten_image(train.image[sel]), train.label[sel])[0]) * len(sel)\n",
        "    avg_loss /= it + len(sel)\n",
        "\n",
        "    # Eval\n",
        "    accuracy = 0\n",
        "    for it in range(0, test.image.shape[0], batch):\n",
        "      x, y = test.image[it: it + batch], test.label[it: it + batch]\n",
        "      accuracy += (np.argmax(predict(model, flatten_image(x)), axis=1) == y).sum()  \n",
        "    accuracy /= test.image.shape[0]\n",
        "    print('Epoch %04d  Loss %.2f  Accuracy %.2f' % (epoch + 1, avg_loss, 100 * accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnrHPajlwHHM",
        "colab_type": "text"
      },
      "source": [
        "## Training the DNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZfMOlGPwudp",
        "colab_type": "code",
        "colab": {
          "height": 1000
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1597866876382,
          "user_tz": 420,
          "elapsed": 253483,
          "user": {
            "displayName": "Alex Kurakin",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggxs3OlaNvrJx7N9pYPsTha3Rztq6Jr89dSgPKY=s64",
            "userId": "14505005774567669519"
          }
        },
        "outputId": "7020faa2-6448-4163-f150-12b4d63e2a58"
      },
      "source": [
        "train_model(dnn_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0001  Loss 2.85  Accuracy 11.35\n",
            "Epoch 0002  Loss 2.16  Accuracy 32.86\n",
            "Epoch 0003  Loss 1.63  Accuracy 47.15\n",
            "Epoch 0004  Loss 1.31  Accuracy 52.86\n",
            "Epoch 0005  Loss 1.21  Accuracy 55.17\n",
            "Epoch 0006  Loss 1.14  Accuracy 58.27\n",
            "Epoch 0007  Loss 1.09  Accuracy 63.68\n",
            "Epoch 0008  Loss 0.97  Accuracy 71.96\n",
            "Epoch 0009  Loss 0.83  Accuracy 74.87\n",
            "Epoch 0010  Loss 0.75  Accuracy 76.94\n",
            "Epoch 0011  Loss 0.73  Accuracy 78.59\n",
            "Epoch 0012  Loss 0.69  Accuracy 79.81\n",
            "Epoch 0013  Loss 0.65  Accuracy 81.46\n",
            "Epoch 0014  Loss 0.61  Accuracy 82.58\n",
            "Epoch 0015  Loss 0.58  Accuracy 83.11\n",
            "Epoch 0016  Loss 0.57  Accuracy 83.69\n",
            "Epoch 0017  Loss 0.55  Accuracy 84.09\n",
            "Epoch 0018  Loss 0.54  Accuracy 84.36\n",
            "Epoch 0019  Loss 0.52  Accuracy 85.24\n",
            "Epoch 0020  Loss 0.50  Accuracy 86.27\n",
            "Epoch 0021  Loss 0.47  Accuracy 86.69\n",
            "Epoch 0022  Loss 0.46  Accuracy 87.49\n",
            "Epoch 0023  Loss 0.44  Accuracy 88.02\n",
            "Epoch 0024  Loss 0.43  Accuracy 87.99\n",
            "Epoch 0025  Loss 0.42  Accuracy 88.31\n",
            "Epoch 0026  Loss 0.42  Accuracy 88.53\n",
            "Epoch 0027  Loss 0.41  Accuracy 88.49\n",
            "Epoch 0028  Loss 0.40  Accuracy 88.77\n",
            "Epoch 0029  Loss 0.39  Accuracy 89.15\n",
            "Epoch 0030  Loss 0.39  Accuracy 89.29\n",
            "Epoch 0031  Loss 0.38  Accuracy 89.27\n",
            "Epoch 0032  Loss 0.38  Accuracy 89.33\n",
            "Epoch 0033  Loss 0.37  Accuracy 89.50\n",
            "Epoch 0034  Loss 0.37  Accuracy 89.44\n",
            "Epoch 0035  Loss 0.37  Accuracy 89.67\n",
            "Epoch 0036  Loss 0.36  Accuracy 89.84\n",
            "Epoch 0037  Loss 0.36  Accuracy 89.89\n",
            "Epoch 0038  Loss 0.36  Accuracy 90.08\n",
            "Epoch 0039  Loss 0.35  Accuracy 89.94\n",
            "Epoch 0040  Loss 0.35  Accuracy 90.06\n",
            "Epoch 0041  Loss 0.36  Accuracy 90.26\n",
            "Epoch 0042  Loss 0.35  Accuracy 90.40\n",
            "Epoch 0043  Loss 0.35  Accuracy 90.37\n",
            "Epoch 0044  Loss 0.34  Accuracy 90.38\n",
            "Epoch 0045  Loss 0.34  Accuracy 90.49\n",
            "Epoch 0046  Loss 0.34  Accuracy 90.64\n",
            "Epoch 0047  Loss 0.34  Accuracy 90.57\n",
            "Epoch 0048  Loss 0.34  Accuracy 90.29\n",
            "Epoch 0049  Loss 0.34  Accuracy 90.59\n",
            "Epoch 0050  Loss 0.33  Accuracy 90.67\n",
            "Epoch 0051  Loss 0.33  Accuracy 90.72\n",
            "Epoch 0052  Loss 0.34  Accuracy 90.70\n",
            "Epoch 0053  Loss 0.33  Accuracy 90.39\n",
            "Epoch 0054  Loss 0.33  Accuracy 90.57\n",
            "Epoch 0055  Loss 0.33  Accuracy 90.80\n",
            "Epoch 0056  Loss 0.33  Accuracy 90.66\n",
            "Epoch 0057  Loss 0.33  Accuracy 90.82\n",
            "Epoch 0058  Loss 0.33  Accuracy 90.78\n",
            "Epoch 0059  Loss 0.33  Accuracy 90.92\n",
            "Epoch 0060  Loss 0.33  Accuracy 91.00\n",
            "Epoch 0061  Loss 0.32  Accuracy 90.99\n",
            "Epoch 0062  Loss 0.33  Accuracy 90.83\n",
            "Epoch 0063  Loss 0.32  Accuracy 90.69\n",
            "Epoch 0064  Loss 0.32  Accuracy 90.89\n",
            "Epoch 0065  Loss 0.32  Accuracy 91.03\n",
            "Epoch 0066  Loss 0.32  Accuracy 90.44\n",
            "Epoch 0067  Loss 0.32  Accuracy 90.96\n",
            "Epoch 0068  Loss 0.32  Accuracy 90.86\n",
            "Epoch 0069  Loss 0.32  Accuracy 91.13\n",
            "Epoch 0070  Loss 0.32  Accuracy 90.96\n",
            "Epoch 0071  Loss 0.32  Accuracy 91.01\n",
            "Epoch 0072  Loss 0.32  Accuracy 90.72\n",
            "Epoch 0073  Loss 0.32  Accuracy 91.29\n",
            "Epoch 0074  Loss 0.32  Accuracy 90.97\n",
            "Epoch 0075  Loss 0.31  Accuracy 91.13\n",
            "Epoch 0076  Loss 0.32  Accuracy 90.60\n",
            "Epoch 0077  Loss 0.31  Accuracy 90.97\n",
            "Epoch 0078  Loss 0.31  Accuracy 91.18\n",
            "Epoch 0079  Loss 0.31  Accuracy 91.09\n",
            "Epoch 0080  Loss 0.32  Accuracy 91.06\n",
            "Epoch 0081  Loss 0.31  Accuracy 91.00\n",
            "Epoch 0082  Loss 0.31  Accuracy 91.31\n",
            "Epoch 0083  Loss 0.31  Accuracy 90.78\n",
            "Epoch 0084  Loss 0.31  Accuracy 91.12\n",
            "Epoch 0085  Loss 0.31  Accuracy 90.93\n",
            "Epoch 0086  Loss 0.32  Accuracy 91.25\n",
            "Epoch 0087  Loss 0.31  Accuracy 91.10\n",
            "Epoch 0088  Loss 0.31  Accuracy 91.24\n",
            "Epoch 0089  Loss 0.31  Accuracy 91.29\n",
            "Epoch 0090  Loss 0.31  Accuracy 91.27\n",
            "Epoch 0091  Loss 0.31  Accuracy 91.26\n",
            "Epoch 0092  Loss 0.31  Accuracy 91.05\n",
            "Epoch 0093  Loss 0.31  Accuracy 91.10\n",
            "Epoch 0094  Loss 0.31  Accuracy 91.09\n",
            "Epoch 0095  Loss 0.31  Accuracy 91.30\n",
            "Epoch 0096  Loss 0.30  Accuracy 91.26\n",
            "Epoch 0097  Loss 0.30  Accuracy 91.18\n",
            "Epoch 0098  Loss 0.31  Accuracy 91.08\n",
            "Epoch 0099  Loss 0.30  Accuracy 91.48\n",
            "Epoch 0100  Loss 0.30  Accuracy 91.27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G31YQExqwNNc",
        "colab_type": "text"
      },
      "source": [
        "## Training the ConvNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz0TmizfSipf",
        "colab_type": "code",
        "colab": {
          "height": 1000
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1597877110571,
          "user_tz": 420,
          "elapsed": 10234184,
          "user": {
            "displayName": "Alex Kurakin",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggxs3OlaNvrJx7N9pYPsTha3Rztq6Jr89dSgPKY=s64",
            "userId": "14505005774567669519"
          }
        },
        "outputId": "d8d17a8a-ebd2-4390-c7bf-2fcb99ba2493"
      },
      "source": [
        "train_model(cnn_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0001  Loss 0.27  Accuracy 54.75\n",
            "Epoch 0002  Loss 0.05  Accuracy 65.24\n",
            "Epoch 0003  Loss 0.03  Accuracy 92.04\n",
            "Epoch 0004  Loss 0.03  Accuracy 93.29\n",
            "Epoch 0005  Loss 0.02  Accuracy 96.63\n",
            "Epoch 0006  Loss 0.02  Accuracy 97.42\n",
            "Epoch 0007  Loss 0.01  Accuracy 89.57\n",
            "Epoch 0008  Loss 0.01  Accuracy 97.12\n",
            "Epoch 0009  Loss 0.01  Accuracy 97.12\n",
            "Epoch 0010  Loss 0.01  Accuracy 97.28\n",
            "Epoch 0011  Loss 0.01  Accuracy 97.85\n",
            "Epoch 0012  Loss 0.01  Accuracy 97.27\n",
            "Epoch 0013  Loss 0.01  Accuracy 98.66\n",
            "Epoch 0014  Loss 0.00  Accuracy 99.11\n",
            "Epoch 0015  Loss 0.00  Accuracy 98.44\n",
            "Epoch 0016  Loss 0.00  Accuracy 98.21\n",
            "Epoch 0017  Loss 0.00  Accuracy 99.12\n",
            "Epoch 0018  Loss 0.00  Accuracy 99.12\n",
            "Epoch 0019  Loss 0.00  Accuracy 99.05\n",
            "Epoch 0020  Loss 0.00  Accuracy 99.16\n",
            "Epoch 0021  Loss 0.00  Accuracy 99.24\n",
            "Epoch 0022  Loss 0.00  Accuracy 99.22\n",
            "Epoch 0023  Loss 0.00  Accuracy 99.09\n",
            "Epoch 0024  Loss 0.00  Accuracy 99.39\n",
            "Epoch 0025  Loss 0.00  Accuracy 99.40\n",
            "Epoch 0026  Loss 0.00  Accuracy 99.45\n",
            "Epoch 0027  Loss 0.00  Accuracy 99.40\n",
            "Epoch 0028  Loss 0.00  Accuracy 99.35\n",
            "Epoch 0029  Loss 0.00  Accuracy 99.44\n",
            "Epoch 0030  Loss 0.00  Accuracy 99.25\n",
            "Epoch 0031  Loss 0.00  Accuracy 99.33\n",
            "Epoch 0032  Loss 0.00  Accuracy 99.34\n",
            "Epoch 0033  Loss 0.00  Accuracy 99.48\n",
            "Epoch 0034  Loss 0.00  Accuracy 99.43\n",
            "Epoch 0035  Loss 0.00  Accuracy 99.45\n",
            "Epoch 0036  Loss 0.00  Accuracy 99.47\n",
            "Epoch 0037  Loss 0.00  Accuracy 99.50\n",
            "Epoch 0038  Loss 0.00  Accuracy 99.47\n",
            "Epoch 0039  Loss 0.00  Accuracy 99.47\n",
            "Epoch 0040  Loss 0.00  Accuracy 99.47\n",
            "Epoch 0041  Loss 0.00  Accuracy 99.48\n",
            "Epoch 0042  Loss 0.00  Accuracy 99.48\n",
            "Epoch 0043  Loss 0.00  Accuracy 99.45\n",
            "Epoch 0044  Loss 0.00  Accuracy 99.38\n",
            "Epoch 0045  Loss 0.00  Accuracy 99.51\n",
            "Epoch 0046  Loss 0.00  Accuracy 99.52\n",
            "Epoch 0047  Loss 0.00  Accuracy 99.44\n",
            "Epoch 0048  Loss 0.00  Accuracy 99.46\n",
            "Epoch 0049  Loss 0.00  Accuracy 99.45\n",
            "Epoch 0050  Loss 0.00  Accuracy 99.42\n",
            "Epoch 0051  Loss 0.00  Accuracy 99.48\n",
            "Epoch 0052  Loss 0.00  Accuracy 99.49\n",
            "Epoch 0053  Loss 0.00  Accuracy 99.40\n",
            "Epoch 0054  Loss 0.00  Accuracy 99.49\n",
            "Epoch 0055  Loss 0.00  Accuracy 99.52\n",
            "Epoch 0056  Loss 0.00  Accuracy 99.44\n",
            "Epoch 0057  Loss 0.00  Accuracy 99.47\n",
            "Epoch 0058  Loss 0.00  Accuracy 99.44\n",
            "Epoch 0059  Loss 0.00  Accuracy 99.36\n",
            "Epoch 0060  Loss 0.00  Accuracy 99.50\n",
            "Epoch 0061  Loss 0.00  Accuracy 99.50\n",
            "Epoch 0062  Loss 0.00  Accuracy 99.46\n",
            "Epoch 0063  Loss 0.00  Accuracy 99.48\n",
            "Epoch 0064  Loss 0.00  Accuracy 99.49\n",
            "Epoch 0065  Loss 0.00  Accuracy 99.45\n",
            "Epoch 0066  Loss 0.00  Accuracy 99.50\n",
            "Epoch 0067  Loss 0.00  Accuracy 99.45\n",
            "Epoch 0068  Loss 0.00  Accuracy 99.45\n",
            "Epoch 0069  Loss 0.00  Accuracy 99.47\n",
            "Epoch 0070  Loss 0.00  Accuracy 99.43\n",
            "Epoch 0071  Loss 0.00  Accuracy 99.48\n",
            "Epoch 0072  Loss 0.00  Accuracy 99.43\n",
            "Epoch 0073  Loss 0.00  Accuracy 99.45\n",
            "Epoch 0074  Loss 0.00  Accuracy 99.49\n",
            "Epoch 0075  Loss 0.00  Accuracy 99.50\n",
            "Epoch 0076  Loss 0.00  Accuracy 99.43\n",
            "Epoch 0077  Loss 0.00  Accuracy 99.35\n",
            "Epoch 0078  Loss 0.00  Accuracy 99.49\n",
            "Epoch 0079  Loss 0.00  Accuracy 99.48\n",
            "Epoch 0080  Loss 0.00  Accuracy 99.47\n",
            "Epoch 0081  Loss 0.00  Accuracy 99.48\n",
            "Epoch 0082  Loss 0.00  Accuracy 99.45\n",
            "Epoch 0083  Loss 0.00  Accuracy 99.49\n",
            "Epoch 0084  Loss 0.00  Accuracy 99.43\n",
            "Epoch 0085  Loss 0.00  Accuracy 99.42\n",
            "Epoch 0086  Loss 0.00  Accuracy 99.48\n",
            "Epoch 0087  Loss 0.00  Accuracy 99.46\n",
            "Epoch 0088  Loss 0.00  Accuracy 99.44\n",
            "Epoch 0089  Loss 0.00  Accuracy 99.44\n",
            "Epoch 0090  Loss 0.00  Accuracy 99.48\n",
            "Epoch 0091  Loss 0.00  Accuracy 99.44\n",
            "Epoch 0092  Loss 0.00  Accuracy 99.45\n",
            "Epoch 0093  Loss 0.00  Accuracy 99.52\n",
            "Epoch 0094  Loss 0.00  Accuracy 99.45\n",
            "Epoch 0095  Loss 0.00  Accuracy 99.42\n",
            "Epoch 0096  Loss 0.00  Accuracy 99.44\n",
            "Epoch 0097  Loss 0.00  Accuracy 99.50\n",
            "Epoch 0098  Loss 0.00  Accuracy 99.49\n",
            "Epoch 0099  Loss 0.00  Accuracy 99.44\n",
            "Epoch 0100  Loss 0.00  Accuracy 99.42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi6kHw1ycwIq",
        "colab_type": "text"
      },
      "source": [
        "## Training with PyTorch data processing API\n",
        "\n",
        "One of the pain points for ML researchers/practioners when building a new ML model is the data processing. Here, we demonstrate how to use data processing API of [PyTorch](https://pytorch.org/) to train a model with Objax. Different deep learning library comes with different data processing APIs, and depending on your preference, you can choose an API and easily combine with Objax.\n",
        "\n",
        "Similarly, we prepare an `MNIST` dataset and apply the same data preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv1enS4rd4FD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform=transforms.Compose([\n",
        "                              transforms.Pad((2,2,2,2), 0),\n",
        "                              transforms.ToTensor(),\n",
        "                              transforms.Lambda(lambda x: np.concatenate([x] * 3, axis=0)),\n",
        "                              transforms.Lambda(lambda x: x * 2 - 1)\n",
        "                              ])\n",
        "train_dataset = datasets.MNIST(os.environ['HOME'], train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(os.environ['HOME'], train=False, download=True, transform=transform)\n",
        "\n",
        "# Define data loader\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNG2rcmylpih",
        "colab_type": "text"
      },
      "source": [
        "We replace data processing pipeline of the train and test loop with `train_loader` and `test_loader` and that's it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94E64UlHd0zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train loop\n",
        "\n",
        "def train_model_with_torch_data_api(model):\n",
        "\n",
        "  def predict(model, x):\n",
        "    \"\"\"\"\"\" \n",
        "    return objax.functional.softmax(model(x,  training=False))\n",
        "    \n",
        "  def flatten_image(x):\n",
        "    \"\"\"Flatten the image before passing it to the DNN.\"\"\"\n",
        "    if isinstance(model, DNNet):\n",
        "      return objax.functional.flatten(x)\n",
        "    else:\n",
        "      return x\n",
        "  \n",
        "  opt = objax.optimizer.Momentum(model.vars())\n",
        "\n",
        "  # Cross Entropy Loss\n",
        "  def loss(x, label):\n",
        "    return objax.functional.loss.cross_entropy_logits_sparse(model(x, training=True), label).mean()\n",
        "\n",
        "  gv = objax.GradValues(loss, model.vars())\n",
        "  def train_op(x, label):\n",
        "    g, v = gv(x, label)  # returns gradients, loss\n",
        "    opt(lr, g)\n",
        "    return v\n",
        "\n",
        "  train_op = objax.Jit(train_op, gv.vars() + opt.vars())  \n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    avg_loss = 0\n",
        "    tot_data = 0\n",
        "    for _, (img, label) in enumerate(train_loader):\n",
        "      avg_loss += float(train_op(flatten_image(img.numpy()), label.numpy())[0]) * len(img)\n",
        "      tot_data += len(img)\n",
        "    avg_loss /= tot_data\n",
        "\n",
        "    # Eval\n",
        "    accuracy = 0\n",
        "    tot_data = 0\n",
        "    for _, (img, label) in enumerate(test_loader):\n",
        "      accuracy += (np.argmax(predict(model, flatten_image(img.numpy())), axis=1) == label.numpy()).sum()\n",
        "      tot_data += len(img)\n",
        "    accuracy /= tot_data\n",
        "    print('Epoch %04d  Loss %.2f  Accuracy %.2f' % (epoch + 1, avg_loss, 100 * accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaUy1lXokCL0",
        "colab_type": "text"
      },
      "source": [
        "## Training the DNN Model with PyTorch data API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWzkHDNJj_QH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dnn_layer_sizes = 3072, 128, 10\n",
        "dnn_model = DNNet(dnn_layer_sizes, objax.functional.leaky_relu)\n",
        "train_model_with_torch_data_api(dnn_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7MkDt3LkZio",
        "colab_type": "text"
      },
      "source": [
        "## Training the ConvNet Model with PyTorch data API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWADCELWkF_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_model = ConvNet(nin=3, nclass=10)\n",
        "cnn_model.vars().print()\n",
        "train_model_with_torch_data_api(cnn_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9WgeoI3wo-o",
        "colab_type": "text"
      },
      "source": [
        "## What's Next\n",
        "\n",
        "We have learned how to use existing models and define new models to classify MNIST. Next, you can read one or more of the in-depth topics or browse through the Objax's APIs."
      ]
    }
  ]
}
